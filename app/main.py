import streamlit as st
from streamlit_option_menu import option_menu

from pygwalker.api.streamlit import StreamlitRenderer
import pandas as pd

from dotenv import load_dotenv
from langchain_teddynote import logging

from utils import print_messages, streamHandler, format_docs, embed_file, get_session_history

from langchain_core.messages import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.runnables import RunnablePassthrough

from langchain_community.chat_models import ChatOllama
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.document_loaders import PyPDFLoader
from langchain import hub
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from operator import itemgetter

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title='DataDriven Management Strategic', page_icon='ğŸ§‘ğŸ»â€ğŸ’»')
st.title('ğŸ“ˆ DataDriven Management Strategic')

# ë§¨ì²˜ìŒ chatbbot chat
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

# ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” store
if 'store' not in st.session_state:
    st.session_state['store'] = dict()

    
with st.sidebar:
    # ê¸°ì¡´ ë©”ë‰´ì— í¬ë¡¤ë§ ë´‡ ì¶”ê°€
    selected = option_menu(
        "Category",
        ["Chat", "Visualization"],
        icons=["robot", "file-earmark-text"],
        menu_icon="cast",
        default_index=0,
        orientation="vertical"
    )
    
    # í¬ë¡¤ë§ ë´‡ ì„ íƒ ì‹œ ì¶”ê°€ ìš”ì†Œ í‘œì‹œ
    if selected == "Chat":        
        # session_id ìƒì„±
        session_id = st.text_input(label='**Chat ID**', value="code")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        file = st.file_uploader(
            "**Data File Upload**",
            type="pdf"
        )
        
        # fileì´ ìˆë‹¤ë©´, embeddied
        if file:
            retriever = embed_file(file)
        
        clear_button = st.button('Reset')
        if clear_button:
            st.session_state['messages'] = []
            st.session_state['store'] = dict()
            st.experimental_rerun()
    else:
        # íŒŒì¼ ì—…ë¡œë“œ
        file = st.file_uploader(
            "**Data File Upload**",
            type="csv"
        )
    

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

if selected == "Chat":
    # ì‚¬ìš©ì ì…ë ¥ì´ ìˆì„ ê²½ìš°
    if user_input := st.chat_input('ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.'):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
        st.chat_message('user').write(f'{user_input}')
        st.session_state['messages'].append(ChatMessage(role='user', content=user_input))
        
        # AI ì‘ë‹µ ì¶œë ¥
        with st.chat_message('assistant'):
            stream_handler = streamHandler(st.empty())
            
            # LLM ìƒì„±
            llm = ChatOllama(
                model='Bllossom-8B-Q4_K_M:latest',
                streaming=True,
                callbacks=[stream_handler],
            )

            if file is not None:
                # PDFê°€ ìˆëŠ” ê²½ìš°ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
                prompt = ChatPromptTemplate.from_messages([
                    ("system",
                    'ë‹¹ì‹ ì€ ì§ˆë¬¸ì— í•œê¸€ë¡œ ë‹µë³€í•˜ëŠ” ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¼ ê²½ìš°, ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ìµœëŒ€ ë‘ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ê³ , ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.'
                    ),
                    MessagesPlaceholder(variable_name="history"),
                    ('human', '{question}')
                ])

                
                # RAG ì²´ì¸ ìƒì„±
                # ì—¬ê¸°ì„œ history ë„£ëŠ”ê²Œ í•µì‹¬.
                rag_chain = (
                    {'context': itemgetter('question') | retriever, "question": RunnablePassthrough(), "history": itemgetter('history')}
                    | prompt
                    | llm
                    | StrOutputParser()
                )
                
                chain_with_memory = RunnableWithMessageHistory(
                    rag_chain,
                    get_session_history,
                    input_messages_key='question',
                    history_messages_key='history'
                )

                # ì„¸ì…˜ë³„ êµ¬ì„±ìœ¼ë¡œ ì²´ì¸ ì‹¤í–‰
                response = chain_with_memory.invoke(
                    {'question': user_input},
                    config={'configurable': {'session_id': session_id}}
                )
                
            # pdf ì…ë ¥ì´ ì—†ì„ ë•Œ (RAG ì‚¬ìš© X)
            else:
                # PDFê°€ ì—†ëŠ” ê²½ìš°ì˜ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
                basic_prompt = ChatPromptTemplate.from_messages([
                    ("system", "ì§ˆë¬¸ì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”."),
                    MessagesPlaceholder(variable_name="history"),
                    ('human', '{question}')
                ])

                # ê¸°ë³¸ ì²´ì¸ ìƒì„±
                basic_chain = (
                    basic_prompt
                    | llm
                    | StrOutputParser()
                )
                
                chain_with_memory = RunnableWithMessageHistory(
                    basic_chain,
                    get_session_history,
                    input_messages_key='question',
                    history_messages_key='history'
                )

                # ì„¸ì…˜ë³„ êµ¬ì„±ìœ¼ë¡œ ì²´ì¸ ì‹¤í–‰
                response = chain_with_memory.invoke(
                    {'question': user_input},
                    config={'configurable': {'session_id': session_id}}
                )

            # AI ì‘ë‹µ ë©”ì‹œì§€
            msg = response
            st.session_state['messages'].append(ChatMessage(role='assistant', content=msg))
else:
    if file is not None:
            # íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°ì—ë§Œ csv íŒŒì¼ ì½ê¸°
            try:
                @st.cache_resource
                def get_pyg_renderer() -> StreamlitRenderer:
                    df = pd.read_csv(file)
                    return StreamlitRenderer(df, spec_io_mode="rw", renderer="svg")
                
                renderer = get_pyg_renderer()
                
                # íƒ­ êµ¬ì„±
                tab1, tab2, tab3 = st.tabs(["Explorer", "Data Profiling", "Charts"])

                with tab1:
                    renderer.explorer(key="explorer_tab1")

                with tab2:
                    renderer.explorer(default_tab="data", key="explorer_tab2")

                with tab3:
                    try:
                        st.subheader("Registered per Weekday")
                        renderer.chart(0)  # 0ë²ˆ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ í•„ìš”

                        st.subheader("Registered per Day")
                        renderer.chart(1)  # 1ë²ˆ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ í•„ìš”
                    except IndexError:
                        st.error("ì°¨íŠ¸ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    except Exception as e:
                        st.error(f"ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            except Exception as e:
                st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.write("")
